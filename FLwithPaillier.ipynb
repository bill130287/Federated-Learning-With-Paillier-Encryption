{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pX38fAVsGN0k"
   },
   "source": [
    "# Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YxS1EKBIAVd"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4A6ymg5MJKHZ"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "# import io\n",
    "# import imageio\n",
    "# from PIL import Image\n",
    "# from PIL import ImageDraw\n",
    "# from PIL import ImageFont\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "#from tqdm import tqdm_notebook as tqdm \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.datasets as dset # 可以透過minibatch的方式，去取得訓練的資料\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "\n",
    "from phe import paillier\n",
    "from multiprocessing import Process, Pool\n",
    "import os, time\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDQDIiDOGi7v"
   },
   "source": [
    "# Download MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpdzLwyBJRzz"
   },
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "# Load it into dataloader\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor()]) \n",
    "train_set = dset.MNIST(root='.', train=True, download=True ,transform=trans)\n",
    "test_set = dset.MNIST(root='.', train=False,transform=trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_key, private_key = paillier.generate_paillier_keypair(n_length=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRsYK2UfHwdT"
   },
   "source": [
    "# Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5EbvRz9qRYEm"
   },
   "outputs": [],
   "source": [
    "class network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(network,self).__init__()\n",
    "        self.L1 = nn.Linear(784,128)\n",
    "        self.L2 = nn.Linear(128,64)\n",
    "        self.output = nn.Linear(64,10)\n",
    "    def forward(self , x):\n",
    "        x = F.relu(self.L1(x))\n",
    "        x = F.relu(self.L2(x))\n",
    "        x = self.output(x)  #若是loss用crossentropy 他最後一層會自己用softmax\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9iyBja58H_n0"
   },
   "source": [
    "# Global Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qYq-jSTFFnd5"
   },
   "outputs": [],
   "source": [
    "def Encrypt_Multithread(data_array, public_key):\n",
    "    encrypt_data_array = np.array([public_key.encrypt(float(x)) for x in data_array.reshape(-1)])\n",
    "    return encrypt_data_array\n",
    "    #encrypted_num_list.append(encrypt_data_array)\n",
    "    #print(f\"Encrypt thread finishes: {i}\")\n",
    "\n",
    "def Decrypt_Multithread(encrypted_num, private_key):\n",
    "    #print(f\"Decrypt thread: {i}\")\n",
    "    decrypted_data_array = np.array([private_key.decrypt(encrypted_num)])\n",
    "    return decrypted_data_array\n",
    "    #decrypted_num_list.append(decrypted_data_array)\n",
    "    #print(f\"Decrypt thread finishes: {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CKg1xfOSV1n"
   },
   "outputs": [],
   "source": [
    "class GlobalServer():\n",
    "    def __init__(self):\n",
    "        def Initial_globalnetwork_dict():\n",
    "            global_network = network()\n",
    "            net_dict = global_network.state_dict()\n",
    "\n",
    "            for key in net_dict.keys():\n",
    "                net_dict[key] = net_dict[key].numpy()\n",
    "\n",
    "            return net_dict\n",
    "\n",
    "        self.global_network_dict = Initial_globalnetwork_dict() #{\"key is layer\": value is numpy(from tensor to numpy)}\n",
    "        self.gradient_buffer = dict()\n",
    "        #self.lr = 0.1\n",
    "        self.loss_history = []\n",
    "\n",
    "  \n",
    "    def AddGradient(self):\n",
    "        state_dict_tmp = self.global_network_dict\n",
    "        for key in self.gradient_buffer.keys():\n",
    "            #print(f\"Add layer:{key}\")\n",
    "            #print(key)\n",
    "            state_dict_tmp[key] += self.gradient_buffer[key]\n",
    "    \n",
    "        self.global_network_dict = state_dict_tmp\n",
    "\n",
    "        self.gradient_buffer.clear()\n",
    "\n",
    "    def Append_loss(self, loss_sum):\n",
    "        self.loss_history.append(loss_sum)\n",
    "\n",
    "    def EncryptParameters(self, public_key):\n",
    "        params_dict = self.global_network_dict\n",
    "\n",
    "        pool = Pool(2)\n",
    "        partial_func = partial(Encrypt_Multithread, public_key=public_key)\n",
    "        for layer in params_dict.keys():\n",
    "            shape = params_dict[layer].shape\n",
    "            params_array = pool.map(partial_func, params_dict[layer].reshape(-1))\n",
    "            params_concatenate = np.concatenate(params_array)\n",
    "            params_dict[layer] = params_concatenate.reshape(shape)\n",
    "\n",
    "        self.global_network_dict = params_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7rTyZwLIMBR"
   },
   "source": [
    "# Participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQZYhWhHVgjM"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class Participant():\n",
    "    def __init__(self):\n",
    "        self.local_network = network()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.lr = 0.1\n",
    "        self.optimizer = torch.optim.SGD(params=self.local_network.parameters(), lr = self.lr)\n",
    "        self.loss_history = []\n",
    "        self.validate_loss = []\n",
    "  # ================================================= Download and upload all gradients =================================================== #\n",
    "    def Download(self, GlobalServer, private_key):\n",
    "        params_dict = GlobalServer.global_network_dict\n",
    "        decrypt_params = dict()\n",
    "        pool = Pool(os.cpu_count())\n",
    "        partial_func = partial(Decrypt_Multithread, private_key=private_key)\n",
    "        for key in params_dict.keys():\n",
    "            shape = params_dict[key].shape\n",
    "            decrypted_array = pool.map(partial_func, params_dict[key].reshape(-1))\n",
    "            decrypt_params[key] = torch.Tensor(np.concatenate(decrypted_array).reshape(shape))\n",
    "\n",
    "        self.local_network.load_state_dict(decrypt_params)\n",
    "\n",
    "  \n",
    "    def Upload(self, gradient_dict, GlobalServer, public_key):\n",
    "        #GlobalServer.gradient_buffer.clear()\n",
    "        pool = Pool(os.cpu_count())\n",
    "        #partial_func = partial(Encrypt_Multithread, public_key=public_key)\n",
    "        for layer in gradient_dict.keys():\n",
    "            shape = gradient_dict[layer].shape\n",
    "            gradients_array = gradient_dict[layer].cpu().numpy()\n",
    "            partial_func = partial(Encrypt_Multithread, public_key=public_key)\n",
    "            encrypted_array = pool.map(partial_func, gradients_array.reshape(-1))\n",
    "            encrypted_concatenate = np.concatenate(encrypted_array)\n",
    "            GlobalServer.gradient_buffer[layer] = encrypted_concatenate.reshape(shape)\n",
    "            #print(f\"Layer:{layer}, GlobalServer:{type(GlobalServer.gradient_buffer[layer])}\")\n",
    "    \n",
    "\n",
    "    def LocalTraining(self, train_dataset, GlobalServer, public_key, private_key, epochs = 1):\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        if use_cuda:\n",
    "            self.local_network = self.local_network.cuda()\n",
    "        tmp_gradient = dict()\n",
    "\n",
    "        for e in tqdm(range(epochs)):\n",
    "            epoch_loss_sum = 0\n",
    "            self.Download(GlobalServer, private_key)\n",
    "      \n",
    "            for x , y in tqdm(train_dataset): # tqdm是可以印進度條的package\n",
    "                ##self.Download(GlobalServer)\n",
    "                #self.optimizer = torch.optim.SGD(params=self.local_network.parameters(), lr = self.lr)\n",
    "                #print(GlobalServer.global_network.state_dict())\n",
    "                #print(self.local_network.state_dict())\n",
    "                if use_cuda:\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "                batch_size = x.shape[0]\n",
    "                x = x.view(batch_size,-1) # 把data tensor的形狀做改變，-1代表由pytorch決定要變多少\n",
    "                net_out = self.local_network(x) # 把x丟進net去train，得到output\n",
    "                loss = self.loss_fn(net_out , y) # 把train出來的結果和ground truth去算loss\n",
    "                epoch_loss_sum += float(loss.item())\n",
    "                 # 做backpropagation\n",
    "                self.optimizer.zero_grad() #先把optimizer清空\n",
    "                loss.backward()\n",
    "                #self.local_network.weight.grad , net.L3.bias.gradnet.L3.weight.grad , net.L3.bias.grad\n",
    "                self.optimizer.step() # 把算完的gradient套在network的parameter\n",
    "                for layer, param in self.local_network.named_parameters():\n",
    "                    if (tmp_gradient.get(layer) == None): \n",
    "                        tmp_gradient[layer] = -1 * param.grad * self.lr\n",
    "                    else:\n",
    "                        tmp_gradient[layer] -= param.grad * self.lr\n",
    "      \n",
    "            #Encrypted_grdd = PaillierEncrypt(tmp_gradient)  \n",
    "            self.Upload(tmp_gradient, GlobalServer, public_key)\n",
    "\n",
    "\n",
    "            GlobalServer.AddGradient()\n",
    "            tmp_gradient.clear()\n",
    "            self.loss_history.append(epoch_loss_sum)\n",
    "            Server.Append_loss(epoch_loss_sum)\n",
    "\n",
    "    def LocalTesting(self, test_dataset):\n",
    "        correct_count = 0\n",
    "        total_testdata = 0\n",
    "        epoch_loss_sum = 0\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        Y = []\n",
    "        Output = []\n",
    "\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        if use_cuda:\n",
    "            self.local_network = self.local_network.cuda()\n",
    "\n",
    "        for x,y in test_dataset:\n",
    "            if use_cuda:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "            batch_size = x.shape[0]\n",
    "            total_testdata += batch_size\n",
    "            x = x.view(batch_size , -1)\n",
    "            output = self.local_network(x).max(1)[1]\n",
    "            net_out = self.local_network(x) \n",
    "\n",
    "            # validation loss\n",
    "            loss = self.loss_fn(net_out , y)\n",
    "            epoch_loss_sum += float(loss.item())\n",
    "\n",
    "            Y = Y + y.tolist()\n",
    "            Output = Output + output.tolist()\n",
    "            correct_count += torch.sum(output==y).item()\n",
    "        correct_count = correct_count\n",
    "        #print(\"Y:\", Y)\n",
    "        #print(\"Output:\", Output)\n",
    "        self.validate_loss.append(epoch_loss_sum)\n",
    "        f1 = f1_score(Y, Output, average = \"macro\")\n",
    "        print('accuracy rate',correct_count/total_testdata)\n",
    "        print(\"f1-score:\", f1)\n",
    "\n",
    "        return correct_count/total_testdata\n",
    "\n",
    "  # ================================================== 分解 Download and (training, upload) Download另外叫 ============================================ #\n",
    "    def Training(self, train_dataset, GlobalServer, public_key, epochs = 1): # 分解Download and Upload\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        if use_cuda:\n",
    "            self.local_network = self.local_network.cuda()\n",
    "\n",
    "        for e in tqdm(range(epochs)):\n",
    "            epoch_loss_sum = 0\n",
    "            tmp_gradient = dict()\n",
    "            for x , y in tqdm(train_dataset): # tqdm是可以印進度條的package\n",
    "                #self.Download(GlobalServer)\n",
    "                #self.optimizer = torch.optim.SGD(params=self.local_network.parameters(), lr = self.lr)\n",
    "                #print(GlobalServer.global_network.state_dict())\n",
    "                #print(self.local_network.state_dict())\n",
    "            if use_cuda:\n",
    "                x = x.cuda()\n",
    "                y = y.cuda()\n",
    "            batch_size = x.shape[0]\n",
    "            x = x.view(batch_size,-1) # 把data tensor的形狀做改變，-1代表由pytorch決定要變多少\n",
    "            net_out = self.local_network(x) # 把x丟進net去train，得到output\n",
    "            loss = self.loss_fn(net_out , y) # 把train出來的結果和ground truth去算loss\n",
    "            epoch_loss_sum += float(loss.item())\n",
    "             # 做backpropagation\n",
    "            self.optimizer.zero_grad() #先把optimizer清空\n",
    "            loss.backward()\n",
    "            #self.local_network.weight.grad , net.L3.bias.gradnet.L3.weight.grad , net.L3.bias.grad\n",
    "            self.optimizer.step() # 把算完的gradient套在network的parameter\n",
    "            for layer, param in self.local_network.named_parameters():\n",
    "                if (tmp_gradient.get(layer) == None): \n",
    "                    tmp_gradient[layer] = -1 * param.grad * self.lr\n",
    "                else:\n",
    "                    tmp_gradient[layer] -= param.grad * self.lr     \n",
    "      \n",
    "            self.Upload(tmp_gradient, GlobalServer, public_key)\n",
    "        \n",
    "            GlobalServer.AddGradient()\n",
    "            tmp_gradient.clear()\n",
    "            self.loss_history.append(epoch_loss_sum)\n",
    "\n",
    "  # ======================================================================== 只有training, Download upload 另外呼叫 ======================================== #\n",
    "    def OnlyTraining(self, train_dataset, GlobalServer, epochs = 1): \n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        if use_cuda:\n",
    "            self.local_network = self.local_network.cuda()\n",
    "\n",
    "        for e in tqdm(range(epochs)):\n",
    "            epoch_loss_sum = 0\n",
    "\n",
    "            for x , y in tqdm(train_dataset): # tqdm是可以印進度條的package\n",
    "                #self.Download(GlobalServer)\n",
    "                #self.optimizer = torch.optim.SGD(params=self.local_network.parameters(), lr = self.lr)\n",
    "                #print(GlobalServer.global_network.state_dict())\n",
    "                #print(self.local_network.state_dict())\n",
    "                if use_cuda:\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "                batch_size = x.shape[0]\n",
    "                x = x.view(batch_size,-1) # 把data tensor的形狀做改變，-1代表由pytorch決定要變多少\n",
    "                net_out = self.local_network(x) # 把x丟進net去train，得到output\n",
    "                loss = self.loss_fn(net_out , y) # 把train出來的結果和ground truth去算loss\n",
    "                epoch_loss_sum += float(loss.item())\n",
    "                 # 做backpropagation\n",
    "                self.optimizer.zero_grad() #先把optimizer清空\n",
    "                loss.backward()\n",
    "                #self.local_network.weight.grad , net.L3.bias.gradnet.L3.weight.grad , net.L3.bias.grad\n",
    "                self.optimizer.step() # 把算完的gradient套在network的parameter\n",
    "        \n",
    "      \n",
    "            self.loss_history.append(epoch_loss_sum)\n",
    "  \n",
    "  # 直接上傳參數\n",
    "    def UploadParameters(self, GlobalServer):\n",
    "        GlobalServer.global_network.load_state_dict(self.local_network.state_dict())\n",
    "\n",
    "  # 直接下載參數 透過layerlist選擇要下載哪幾層的\n",
    "    def DownloadLayer(self, Server, layer_list):\n",
    "        #network_dict = dict()\n",
    "        local_network_dict = self.local_network.state_dict()\n",
    "        server_netwrok_dict = Server.global_network.state_dict()\n",
    "\n",
    "        for layer in layer_list:\n",
    "            local_network_dict[layer] = server_netwrok_dict[layer]\n",
    "    \n",
    "        self.local_network.load_state_dict(local_network_dict)\n",
    "  \n",
    "  # 上傳gradient 透過layerlist選擇要上傳哪幾層\n",
    "    def UploadLayer(self, gradient_dict, GlobalServer, layer_list):\n",
    "        GlobalServer.gradient_buffer.clear()\n",
    "    \n",
    "        for layer in layer_list:\n",
    "            GlobalServer.gradient_buffer[layer] = -1 * gradient_dict[layer] * self.lr\n",
    "    \n",
    "        #GlobalServer.lr = self.lr\n",
    "        #print(GlobalServer.gradient_buffer)\n",
    "  \n",
    "  # Training 透過layer_list看要上傳哪幾層的gradient Download要在前面呼叫\n",
    "    def TrainingUploadLayer(self, train_dataset, GlobalServer, layer_list, epochs = 1):\n",
    "        use_cuda = torch.cuda.is_available()\n",
    "        if use_cuda:\n",
    "            self.local_network = self.local_network.cuda()\n",
    "\n",
    "        for e in tqdm(range(epochs)):\n",
    "            epoch_loss_sum = 0\n",
    "            tmp_gradient = dict()\n",
    "            for x , y in tqdm(train_dataset): # tqdm是可以印進度條的package\n",
    "                #self.Download(GlobalServer)\n",
    "                #self.optimizer = torch.optim.SGD(params=self.local_network.parameters(), lr = self.lr)\n",
    "                #print(GlobalServer.global_network.state_dict())\n",
    "                #print(self.local_network.state_dict())\n",
    "                if use_cuda:\n",
    "                    x = x.cuda()\n",
    "                    y = y.cuda()\n",
    "                batch_size = x.shape[0]\n",
    "                x = x.view(batch_size,-1) # 把data tensor的形狀做改變\n",
    "                net_out = self.local_network(x) # 把x丟進net去train，得到output\n",
    "                loss = self.loss_fn(net_out , y) # 把train出來的結果和ground truth去算loss\n",
    "                epoch_loss_sum += float(loss.item())\n",
    "                 # 做backpropagation\n",
    "                self.optimizer.zero_grad() #先把optimizer清空\n",
    "                loss.backward()\n",
    "                #self.local_network.weight.grad , net.L3.bias.gradnet.L3.weight.grad , net.L3.bias.grad\n",
    "                self.optimizer.step() # 把算完的gradient套在network的parameter\n",
    "                for layer, param in zip(self.local_network.state_dict().keys(), self.local_network.parameters()):\n",
    "                    if (tmp_gradient.get(layer) == None): \n",
    "                        tmp_gradient[layer] = -1 * param.grad\n",
    "                    else:\n",
    "                        tmp_gradient[layer] -= param.grad\n",
    "\n",
    "            self.UploadLayer(tmp_gradient, GlobalServer, layer_list)\n",
    "        \n",
    "            GlobalServer.AddGradient()\n",
    "            tmp_gradient.clear()\n",
    "            self.loss_history.append(epoch_loss_sum)\n",
    "  \n",
    "  # 直接上傳某一層參數\n",
    "    def UploadParametersLayer(self, GlobalServer, layer_list):\n",
    "        global_state_dict = GlobalServer.global_network.state_dict()\n",
    "        local_state_dict = self.local_network.state_dict()\n",
    "        for layer in layer_list:\n",
    "            global_state_dict[layer] = local_state_dict[layer]\n",
    "    \n",
    "        GlobalServer.global_network.load_state_dict(global_state_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKQ9FvnoI_CS"
   },
   "source": [
    "# Generate Participant and Assign Dataset To Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upSgguLXFZSU"
   },
   "outputs": [],
   "source": [
    "def GenerateParticipant(num = 1):\n",
    "    P_dict = dict()\n",
    "    for i in range(num):\n",
    "        s = 'P'\n",
    "        s = s + str(i)\n",
    "        P_dict[s] = Participant()\n",
    "\n",
    "    return P_dict\n",
    "\n",
    "def SplitData(train_set, test_set, num):\n",
    "    train_split = int(len(train_set) / num)\n",
    "    test_split = int(len(test_set) / num)\n",
    "\n",
    "    portions = [train_split] * num\n",
    "    TrainSet_list = [None] * num\n",
    "    TrainSet_list = data.random_split(train_set, portions)\n",
    "\n",
    "    portions = [test_split] * num\n",
    "    TestSet_list = [None] * num\n",
    "    TestSet_list = data.random_split(test_set, portions)\n",
    "\n",
    "    TrainDataSet_dict = dict()\n",
    "    TestDataSet_dict = dict()\n",
    "\n",
    "    # mini batch\n",
    "    for i in range(num):\n",
    "        s = 'P'\n",
    "        s = s + str(i)\n",
    "        TrainDataSet_dict[s] = data.DataLoader(dataset =  TrainSet_list[i], batch_size=50, shuffle=True)\n",
    "        TestDataSet_dict[s] = data.DataLoader(dataset =  TestSet_list[i], batch_size=50, shuffle=True)\n",
    "\n",
    "    return TrainDataSet_dict, TestDataSet_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agGnmzYxYuQq"
   },
   "source": [
    "# DSSGD\n",
    "Reference: https://www.cs.cornell.edu/~shmat/shmat_ccs15.pdf, https://ieeexplore.ieee.org/document/8241854"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#測試準確率的 test dataset (全部10,000的資料)\n",
    "test_dataset = data.DataLoader(dataset =  test_set, batch_size=50,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlXmZ_ClYwQB"
   },
   "source": [
    "## Round Robin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvY3CZjLYtXr"
   },
   "outputs": [],
   "source": [
    "# 用來檢測accuracy\n",
    "test_P = GenerateParticipant(1) \n",
    "\n",
    "# Dataset and Participant Gernerate\n",
    "n = 10\n",
    "Participant_dict = GenerateParticipant(n)\n",
    "TrainData_dict, TestData_dict = SplitData(train_set, test_set, n)\n",
    "\n",
    "Server = GlobalServer()\n",
    "Server.EncryptParameters(public_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jrxVFdhZOmW"
   },
   "outputs": [],
   "source": [
    "global_accuracy_list = []\n",
    "global_loss_history = []\n",
    "rounds = 1 # round robin要做幾輪\n",
    "\n",
    "for i in tqdm(range(rounds)):\n",
    "    for j in range(n):\n",
    "        s = 'P'\n",
    "        s = s + str(j)\n",
    "        #Participant_dict[s].Download(Server, private_key)\n",
    "        #Participant_dict[s].Training(TrainData_dict[s], Server, public_key, 1)\n",
    "        Participant_dict[s].LocalTraining(TrainData_dict[s], Server, public_key, private_key, 1)\n",
    "        test_P[\"P0\"].Download(Server, private_key)\n",
    "        accu = test_P[\"P0\"].LocalTesting(test_dataset)\n",
    "        global_accuracy_list.append(accu)\n",
    "        global_loss_history.append(Participant_dict[s].loss_history[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGp7QLKDZxa5"
   },
   "source": [
    "### 怕google colab當掉 所以要測10次取平均的時候一次一次做"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sEmZxNwtze1k"
   },
   "outputs": [],
   "source": [
    "# 只有第一次要generate 剩下不要run 不然會蓋掉前面資料\n",
    "global_accuracy_array = np.empty((10, 100))\n",
    "global_loss_array = np.empty((10, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wezhdUsr1VoC"
   },
   "outputs": [],
   "source": [
    "global_accuracy_array[0][:] = np.array(global_accuracy_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jViKyklI1cVv"
   },
   "outputs": [],
   "source": [
    "global_loss_array[0][:] = np.array(global_loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVIhhZYmp_Zl"
   },
   "source": [
    "## Asychronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9HefHjmL8zF"
   },
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "up_cnt = 0\n",
    "\n",
    "k = 1 # 上一個participant訓練完後 接下來要幾(k)個participan去訓練並上傳\n",
    "rounds = 1 # 要做幾輪\n",
    "m_list = list(range(9, 10)) # 不同的m，因為怕google colab當掉，所以建議一個一個去試 尤其是epoch = 10 epcoh = 1還好\n",
    "\n",
    "epoch_set = [1]#因為怕當掉所以分別去試 [1, 10] # 每個participant經過的epoch，Server經過幾個epoch要再乘上有幾個participant\n",
    "\n",
    "asy_accuracy_dict = dict() # key是代表epoch為多少 value為list index由0~9分別為m = 1~10\n",
    "\n",
    "\n",
    "up_cnt = 0 # 第幾個participant\n",
    "#accu_list = []\n",
    "accu_sum = 0\n",
    "average_accu = 0\n",
    "\n",
    "times = 1 #不同的m要做幾次取平均，因為怕colab掛掉，所以這邊設1，每次把數據紀錄起來\n",
    "\n",
    "for m in m_list:\n",
    "    for _ in range(times):\n",
    "        n = 10\n",
    "        Participant_dict = GenerateParticipant(n)\n",
    "        TrainData_dict, TestData_dict = SplitData(train_set, test_set, n)\n",
    "\n",
    "        Server = GlobalServer()\n",
    "        Server.EncryptParameters(public_key)\n",
    "\n",
    "        #cnt = 0\n",
    "        #up_cnt = 0\n",
    "        Participant_book = list(Participant_dict.keys())\n",
    "        P = GenerateParticipant(1)\n",
    "        for _ in range(rounds):\n",
    "            cnt = 0\n",
    "            up_cnt = 0\n",
    "            Upload_list = random.sample(Participant_book, n)\n",
    "            for key in Upload_list:\n",
    "                #print(key)\n",
    "                for i in range(m):\n",
    "                    if (cnt == n):\n",
    "                        break\n",
    "                    #print(cnt)\n",
    "                    Participant_dict[Upload_list[cnt]].Download(Server, private_key)\n",
    "                    cnt += 1\n",
    "\n",
    "                for _ in range(k):\n",
    "                    if (up_cnt == n):\n",
    "                        break \n",
    "                    Participant_dict[Upload_list[up_cnt]].Training(TrainData_dict[Upload_list[up_cnt]], Server, public_key, 1)\n",
    "                    up_cnt += 1\n",
    "            #print(round)\n",
    "        P[\"P0\"].Download(Server, private_key)\n",
    "        accu_sum += P[\"P0\"].LocalTesting(test_dataset)\n",
    "        #accu_list.append(P[\"P0\"].LocalTesting(test_dataset))\n",
    "        #print(cnt)\n",
    "        #accu_list.clear()\n",
    "    average_accu = accu_sum / times #除上做幾次\n",
    "    if (asy_accuracy_dict.get('m'+str(m)+'totalepochs:'+str(r * n)) == None):\n",
    "        asy_accuracy_dict['m'+str(m)+'totalepochs:'+str(r * n)] = []\n",
    "\n",
    "    asy_accuracy_dict['m'+str(m)+'totalepochs:'+str(r * n)].append(average_accu)\n",
    "    accu_sum = 0\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial\n",
    "n = 10\n",
    "Participant_dict = GenerateParticipant(n)\n",
    "TrainData_dict, TestData_dict = SplitData(train_set, test_set, n)\n",
    "\n",
    "Server = GlobalServer()\n",
    "Server.EncryptParameters(public_key)\n",
    "\n",
    "Participant_book = list(Participant_dict.keys())\n",
    "Upload_list = random.sample(Participant_book, n)\n",
    "\n",
    "# 驗證準確性\n",
    "test_P = GenerateParticipant()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_accuracy_list = []\n",
    "loss_history = []\n",
    "rounds = 1\n",
    "\n",
    "for i in tqdm(range(rounds)):\n",
    "    #Upload_list = random.sample(Participant_book, n)\n",
    "    for key in Upload_list:\n",
    "        Participant_dict[key].Download(Server)\n",
    "        Participant_dict[key].OnlyTraining(TrainData_dict[key], Server, 1)\n",
    "        Participant_dict[key].UploadParameters(Server)\n",
    "        test_P[\"P0\"].Download(Server)\n",
    "        accu = test_P[\"P0\"].LocalTesting(test_dataset)\n",
    "        global_accuracy_list.append(accu)\n",
    "        loss_history.append(Participant_dict[key].loss_history[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asychronous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1 # 上一部分participants download完後 接下來要幾個participants去訓練並上傳\n",
    "rounds = 1 # 要做幾輪\n",
    "m_list = list(range(10, 11)) # 不同的m，因為我用google colab, 怕google colab當掉，所以我是一個一個去試 \n",
    "\n",
    "#epoch_set = [1] # 每個participant做完1 epoch上傳奇gradients\n",
    "\n",
    "asy_accuracy_dict = dict() # key是代表Server經過的總epochs為多少 value為分別為m = 1~10的accuracy\n",
    "#asy_accuracy_dict[\"m4totalepochs:100\"] = []\n",
    "#asy_accuracy_dict[\"e10\"] = []\n",
    "\n",
    "\n",
    "#accu_list = []\n",
    "accu_sum = 0\n",
    "average_accu = 0\n",
    "\n",
    "times = 1 #要做幾次去測試\n",
    "epochs = 1 # 每個participants做幾個epoch後上傳\n",
    "\n",
    "for m in m_list:\n",
    "    for _ in range(times):\n",
    "        n = 10\n",
    "        Participant_dict = GenerateParticipant(n)\n",
    "        TrainData_dict, TestData_dict = SplitData(train_set, test_set, n)\n",
    "        Server = GlobalServer()\n",
    "        Server.EncryptParameters(public_key)\n",
    "        up_cnt = 0\n",
    "        cnt = 0\n",
    "        Participant_book = list(Participant_dict.keys())\n",
    "        P = GenerateParticipant(1) # 檢測global server的accuracy\n",
    "        for _ in range(rounds):\n",
    "            cnt = 0\n",
    "            up_cnt = 0\n",
    "            Upload_list = random.sample(Participant_book, n)\n",
    "\n",
    "            for key in Upload_list:\n",
    "                Participant_dict[key].Download(Server)\n",
    "                Participant_dict[key].OnlyTraining(TrainData_dict[key], Server, epochs)\n",
    "                cnt += 1\n",
    "                if cnt % m == 0:\n",
    "                    Participant_dict[Upload_list[up_cnt]].UploadParameters(Server)\n",
    "                up_cnt += 1\n",
    "            print(up_cnt)   \n",
    "      \n",
    "            for _ in range(n - up_cnt):\n",
    "                Participant_dict[Upload_list[up_cnt]].UploadParameters(Server)\n",
    "                #Server.AddGradient()\n",
    "                up_cnt += 1\n",
    "\n",
    "            P[\"P0\"].Download(Server)\n",
    "            accu_sum += P[\"P0\"].LocalTesting(test_dataset)\n",
    "      \n",
    "    average_accu = accu_sum / times #除上做幾次\n",
    "    if (asy_accuracy_dict.get('m'+str(m)+'totalepochs:'+str(r * n * epochs)) == None):\n",
    "        asy_accuracy_dict['m'+str(m)+'totalepochs:'+str(r * n * epochs)] = []\n",
    "\n",
    "    asy_accuracy_dict['m'+str(m)+'totalepochs:'+str(r * n * epochs)].append(average_accu)\n",
    "    accu_sum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPNkhXdCLsWZpbRYws7O7iT",
   "collapsed_sections": [],
   "name": "FLwithPaillier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
